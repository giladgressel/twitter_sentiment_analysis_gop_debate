{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOP Dataset has 13871 rows, 21 columns\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "sns.set(color_codes = True)\n",
    "\n",
    "# Tell iPython to include plots inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "data_gop = pd.read_csv('gop_Sentiment.csv')\n",
    "print \"GOP Dataset has {} rows, {} columns\".format(*data_gop.shape)  # * unpacks the shape tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' id', 0), ('candidate', 1), ('candidate_confidence', 2), ('relevant_yn', 3), ('relevant_yn_confidence', 4), ('sentiment', 5), ('sentiment_confidence', 6), ('subject_matter', 7), ('subject_matter_confidence', 8), ('candidate_gold', 9), ('name', 10), ('relevant_yn_gold', 11), ('retweet_count', 12), ('sentiment_gold', 13), ('subject_matter_gold', 14), ('text', 15), ('tweet_coord', 16), ('tweet_created', 17), ('tweet_id', 18), ('tweet_location', 19), ('user_timezone', 20)]\n"
     ]
    }
   ],
   "source": [
    "#A lot of information we really aren't interested in, so take a look at the columns with their index.\n",
    "header_index = [(i,z) for z,i in enumerate(data_gop.columns.view())]\n",
    "print header_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment                                               text\n",
      "0   Neutral  RT @NancyLeeGrahn: How did everyone feel about...\n",
      "1  Positive  RT @ScottWalker: Didn't catch the full #GOPdeb...\n",
      "2   Neutral  Re-SubmissionT @TJMShow: No mention of Tamir R...\n",
      "3  Positive  RT @RobGeorge: That Carly Fiorina is trending ...\n",
      "4  Positive  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...\n"
     ]
    }
   ],
   "source": [
    "#Now we can drop the ones we won't be using. We are keeping ID-0, sentiment-5,  text-15.\n",
    "#set the index to ' id' and the the other two columns as sentiment and tweet\n",
    "df_gop = pd.DataFrame(index = data_gop[' id'])\n",
    "df_gop = data_gop[['sentiment','text']].copy()\n",
    "print df_gop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neutral' 'Positive' 'Negative']\n",
      "\n",
      "Columns have null values?\n",
      "sentiment    False\n",
      "text         False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "#let's look for what kinds of unique values we have in our sentiment scoring category.\n",
    "# We should also check if we have NaN's that we'd want to deal with\n",
    "print df_gop.sentiment.unique()\n",
    "print \"\"\n",
    "print \"Columns have null values?\"\n",
    "print df_gop.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11378e250>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAERCAYAAACO6FuTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSv8PTdqCPOvm48UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10be603d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot( x = 'sentiment' , data = df_gop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gop tweets \n",
      "\n",
      "   sentiment                                               text\n",
      "0          0  RT @NancyLeeGrahn: How did everyone feel about...\n",
      "1          1  RT @ScottWalker: Didn't catch the full #GOPdeb...\n",
      "2          0  Re-SubmissionT @TJMShow: No mention of Tamir R...\n",
      "3          1  RT @RobGeorge: That Carly Fiorina is trending ...\n",
      "4          1  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...\n",
      "5          1  RT @GregAbbott_TX: @TedCruz: \"On my first day ...\n",
      "6         -1  RT @warriorwoman91: I liked her and was happy ...\n",
      "7          0  Going on #MSNBC Live with @ThomasARoberts arou...\n",
      "8         -1  Deer in the headlights RT @lizzwinstead: Ben C...\n",
      "9         -1  RT @NancyOsborne180: Last night's debate prove...\n"
     ]
    }
   ],
   "source": [
    "#we will want our labels numeric (some classifiers may not like 3-way text labels),\n",
    "#we can do that all now.\n",
    "\n",
    "df_gop.sentiment.replace(['Positive', 'Negative', 'Neutral'], [1, -1 , 0], inplace = True)\n",
    "print \"gop tweets \\n\"\n",
    "print df_gop[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of samples is : 13871\n",
      "There are 2236 positive tweets or 0.161199625117%\n",
      "There are 8493 Negative tweets or 0.612284622594%\n",
      "There are 3142 Neutral tweets or 0.226515752289%\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at our class distribution\n",
    "total_tweets = len(df_gop)\n",
    "positive_tweets = sum(df_gop.sentiment == 1)\n",
    "negative_tweets = sum(df_gop.sentiment == -1)\n",
    "neutral_tweets = sum(df_gop.sentiment == 0)\n",
    "\n",
    "print \"The total number of samples is : {}\".format(len(df_gop.sentiment))\n",
    "print \"There are {} positive tweets or {}%\".format \\\n",
    "(positive_tweets, positive_tweets/float(total_tweets) )\n",
    "print \"There are {} Negative tweets or {}%\".format \\\n",
    "(negative_tweets, negative_tweets / float(total_tweets))\n",
    "print \"There are {} Neutral tweets or {}%\".format \\\n",
    "(neutral_tweets, neutral_tweets/ float(total_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Tweets\n",
    "We need to do a few steps before we can use the tweets to produce features.\n",
    "- parse the tweets into single words\n",
    "- remove retweets, these don't add any sentiment value\n",
    "- remove urls, same as above.\n",
    "- remove stop-words same as above\n",
    "\n",
    "I am passing the bulk of the parsing to the ARK tokenizer from CMU, in this case we are using a python port of the original Java.  link to the port is : https://github.com/myleott/ark-twokenize-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @NancyLeeGrahn: How did everyone feel about the Climate Change question last night? Exactly. #GOPDebate\n",
      "RT @ScottWalker: Didn't catch the full #GOPdebate last night. Here are some of Scott's best lines in 90 seconds. #Walker16 http://t.co/ZSfFâ€¦\n",
      "Re-SubmissionT @TJMShow: No mention of Tamir Rice and the #GOPDebate was held in Cleveland? Wow.\n",
      "\n",
      "['RT', '@NancyLeeGrahn', ':', 'How', 'did', 'everyone', 'feel', 'about', 'the', 'Climate', 'Change', 'question', 'last', 'night', '?', 'Exactly', '.', '#GOPDebate']\n",
      "['RT', '@ScottWalker', ':', \"Didn't\", 'catch', 'the', 'full', '#GOPdebate', 'last', 'night', '.', 'Here', 'are', 'some', 'of', \"Scott's\", 'best', 'lines', 'in', '90', 'seconds', '.', '#Walker16', 'http://t.co/ZSfF\\xe2\\x80\\xa6']\n",
      "['Re-SubmissionT', '@TJMShow', ':', 'No', 'mention', 'of', 'Tamir', 'Rice', 'and', 'the', '#GOPDebate', 'was', 'held', 'in', 'Cleveland', '?', 'Wow', '.']\n"
     ]
    }
   ],
   "source": [
    "# ok let's parse some sample the tweets use the ARK tokenizer from CMU.\n",
    "import twokenize as tw\n",
    "\n",
    "first_tweet = df_gop.iloc[0, 1]\n",
    "second_tweet = df_gop.iloc[1,1]\n",
    "third_tweet = df_gop.iloc[2,1]\n",
    "\n",
    "test_tweets = [first_tweet, second_tweet, third_tweet]\n",
    "for tweet in test_tweets:\n",
    "    print tweet\n",
    "\n",
    "print \"\"\n",
    "\n",
    "for tweet in test_tweets:\n",
    "    print tw.tokenizeRawTweetText(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK that works for a rough parse well enough, let's remove the stop words, RT's and links now.  The words found in stopwords.txt came from this site:\n",
    "http://www.ranks.nl/stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", \"i'm\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "#stop-word removal\n",
    "with open('stopwords.txt') as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "print stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everyone', 'feel', 'climate', 'change', 'question', 'last', 'night', '?', 'exactly', '#gopdebate']\n",
      "['didnt', 'catch', 'full', '#gopdebate', 'last', 'night', 'scotts', 'best', 'lines', '90', 'seconds', '#walker16', '\\xe2\\x80\\xa6']\n",
      "['re-submissiont', '@tjmshow', 'mention', 'tamir', 'rice', '#gopdebate', 'held', 'cleveland', '?', 'wow']\n"
     ]
    }
   ],
   "source": [
    "# compile a regex for stop words, urls, rt's, etc\n",
    "\n",
    "retweets = re.compile(r'(rt ?@.*?:)')   \n",
    "urls = re.compile(r'(https?:.*\\b)')\n",
    "extras = re.compile(r\"\\.|\\.\\.|I|The|:|'|,|\\\"\")\n",
    "stop_word = re.compile(r'\\b({})\\b'.format('|'.join(stop_words)))\n",
    "\n",
    "regex_args = [retweets, urls, extras,  stop_word]\n",
    "#make a method to easily use our regex : note we do not want to compile the regex within this def, that would be computationally expensive.\n",
    "def parse_tweet (tweet , *args):\n",
    "    tweet = tweet.lower()\n",
    "    for arg in args:\n",
    "        tweet = re.sub(arg, \"\", tweet)\n",
    "    tweet = tw.tokenizeRawTweetText(tweet)\n",
    "    if len(tweet) < 1:\n",
    "        return \"NaN\"\n",
    "    else:\n",
    "        return tweet\n",
    "\n",
    "for tweet in test_tweets:\n",
    "    print parse_tweet(tweet, *regex_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok that's working well enough, let's parse all the tweets in the data frame now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                               text\n",
      "0          0  [everyone, feel, climate, change, question, la...\n",
      "1          1  [didnt, catch, full, #gopdebate, last, night, ...\n",
      "2          0  [re-submissiont, @tjmshow, mention, tamir, ric...\n",
      "3          1  [carly, fiorina, trending, --, hours, debate, ...\n",
      "4          1  [#gopdebate, w/, @realdonaldtrump, delivered, ...\n",
      "(13871, 2)\n"
     ]
    }
   ],
   "source": [
    "#parse all tweets in dataframe\n",
    "# ok, now that we have rough parsing, lets parse them all!\n",
    "#run a decoder on unicode, cause \n",
    "df_gop.text = df_gop.text.apply(lambda x: x.decode('utf-8'))\n",
    "df_gop.text = df_gop.text.apply(lambda x: parse_tweet(x,*regex_args))\n",
    "print df_gop.head()\n",
    "print df_gop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment    False\n",
      "text         False\n",
      "dtype: bool\n",
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# let's check to make sure we didnt' parse anything down to an empty string or became NaN\n",
    "print df_gop.isnull().any()\n",
    "print np.where(df_gop.applymap(lambda x: x == ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training tweets:  10403\n",
      "size of testing tweets:  3468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA38AAAGJCAYAAAA644qiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVIB/j8WVGvhTk4x/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113fcacd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now that we have all the tweets parsed, we actually want to split into our training / testing sets. \n",
    "# This is because n-gram analysis (which comes next), should not be done on the testing data!  \n",
    "# The n-gram analysis should on be on training data.  \n",
    "\n",
    "# TODO try to implement n-gram analysis with cross validation, for now I'll use a hold-out testing set\n",
    "from sklearn import cross_validation\n",
    "\n",
    "#Let's split up the labels from the training data\n",
    "\n",
    "X_all = df_gop['text']\n",
    "y_all = df_gop['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "        X_all, y_all, test_size=0.25, stratify = y_all, random_state = 0)  #note we run stratify, in order to try and keep a balanced labeled set\n",
    "\n",
    "print \"size of training tweets: \", len(X_train)\n",
    "print \"size of testing tweets: \", len(X_test)\n",
    "\n",
    "plt.figure(figsize = (15,6))\n",
    "plt.subplot(1,2,1)\n",
    "ax = sns.countplot(y_train)\n",
    "ax.set_title(\"Training Labels\")\n",
    "plt.subplot(1,2,2)\n",
    "ax = sns.countplot(y_test)\n",
    "ax.set_title(\"Testing Labels\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'ok', u'can', u'cull', u'herd', u'two', u'#gopdebates', u'?', u'can', u'tell', u'pataki', u'carson', u'walker', u'cruz', u'perry', u'go', u'home', u'?']\n",
      "                                                    text  sentiment\n",
      "10194  [ok, can, cull, herd, two, #gopdebates, ?, can...         -1\n",
      "1811   [dont, like, @megynkelly, will, defend, obnoxi...          1\n",
      "4223   [can, just, build, fence, around, trump, ?, #g...         -1\n",
      "12201  [#plannedparenthood, funded, florida, -, flori...         -1\n",
      "8859   [many, great, jokes, twitter, tonight, #gopdeb...         -1\n"
     ]
    }
   ],
   "source": [
    "# we need to merge the labels and parsed tweets for doing our n-gram analysis.\n",
    "# This is because we will build n-gram models for each class, therefore we need to select only\n",
    "# those tweets that are positive / negative for the two n-gram tables.\n",
    "\n",
    "# Let's re-merge the labels into the training data order to do n-gram analysis\n",
    "XyN_gram = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "print XyN_gram.iloc[0, 0]\n",
    "print XyN_gram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "#Let's start by developing a function that will take a parsed tweet and count the number of grams of any size\n",
    "def nGram_counter (parsed_tweet, distance_to_cover, gram_map):\n",
    "    for_loop_range = range(len(parsed_tweet) - distance_to_cover)    \n",
    "    for i in for_loop_range:\n",
    "        gram = tuple(parsed_tweet[i:i+distance_to_cover])\n",
    "        gram_map[gram] +=1\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test tweet: [u'ok', u'can', u'cull', u'herd', u'two', u'#gopdebates', u'?', u'can', u'tell', u'pataki', u'carson', u'walker', u'cruz', u'perry', u'go', u'home', u'?']\n",
      "\n",
      "defaultdict(<type 'int'>, {(u'herd',): 1, (u'can',): 2, (u'cull',): 1, (u'tell',): 1, (u'perry',): 1, (u'?',): 1, (u'ok',): 1, (u'go',): 1, (u'pataki',): 1, (u'#gopdebates',): 1, (u'carson',): 1, (u'cruz',): 1, (u'two',): 1, (u'walker',): 1, (u'home',): 1})\n",
      " \n",
      "defaultdict(<type 'int'>, {(u'perry', u'go'): 1, (u'herd', u'two'): 1, (u'cruz', u'perry'): 1, (u'can', u'cull'): 1, (u'cull', u'herd'): 1, (u'#gopdebates', u'?'): 1, (u'tell', u'pataki'): 1, (u'can', u'tell'): 1, (u'two', u'#gopdebates'): 1, (u'go', u'home'): 1, (u'?', u'can'): 1, (u'ok', u'can'): 1, (u'carson', u'walker'): 1, (u'pataki', u'carson'): 1, (u'walker', u'cruz'): 1})\n",
      " \n",
      "defaultdict(<type 'int'>, {(u'walker', u'cruz', u'perry'): 1, (u'can', u'cull', u'herd'): 1, (u'ok', u'can', u'cull'): 1, (u'herd', u'two', u'#gopdebates'): 1, (u'can', u'tell', u'pataki'): 1, (u'pataki', u'carson', u'walker'): 1, (u'#gopdebates', u'?', u'can'): 1, (u'tell', u'pataki', u'carson'): 1, (u'cull', u'herd', u'two'): 1, (u'two', u'#gopdebates', u'?'): 1, (u'perry', u'go', u'home'): 1, (u'carson', u'walker', u'cruz'): 1, (u'cruz', u'perry', u'go'): 1, (u'?', u'can', u'tell'): 1})\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# ok let's try out the above function and make sure it's doing what we think\n",
    "# we will use the same test tweet as before \n",
    "# test1 = [u'ok', u',', u'can', u'cull', u'herd', u'two', u'#gopdebates', u'?', u'can', u'tell', u'pataki', u',', u'carson', u',', u'walker', u',', u'cruz', u'perry', u'go', u'home', u'?']\n",
    "\n",
    "test1 = XyN_gram.iloc[0,0]\n",
    "\n",
    "test_uni = defaultdict(int)\n",
    "test_bi = defaultdict(int)\n",
    "test_tri = defaultdict(int)\n",
    "\n",
    "nGram_counter(test1, 3, test_tri)\n",
    "nGram_counter(test1, 2, test_bi)\n",
    "nGram_counter(test1, 1, test_uni)\n",
    "# check our output.\n",
    "print \"the test tweet: {}\".format(test1)\n",
    "print \"\"\n",
    "print test_uni\n",
    "print \" \"\n",
    "print test_bi\n",
    "print \" \"\n",
    "print test_tri\n",
    "print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's now apply our n-gram counter to all the tweets of a certain class.\n",
    "\n",
    "def n_gram_counter (tweets, map1, map2, map3, classtype):\n",
    "    tweets.apply(lambda x: nGram_counter(x.text, 1, map1), 1)\n",
    "    tweets.apply(lambda x: nGram_counter(x.text, 2, map2), 1)\n",
    "    tweets.apply(lambda x: nGram_counter(x.text, 3, map3), 1)\n",
    "    print \"Total Unigrams for {} Tweets : {}\".format(classtype, len(map1))\n",
    "    print \n",
    "    print \"Total Bi-grams for {} Tweets: {}\".format(classtype, len(map2))\n",
    "    print\n",
    "    print \"Total Tri-grams for {} Tweets: {}\".format(classtype, len(map3))\n",
    "    print\n",
    "    print \"Most popular {} Uni-grams : {}\" \\\n",
    "    .format(classtype, sorted(map1.items(), key=lambda x: x[1], reverse = True)[:15])\n",
    "    print\n",
    "    print \"Most Popular {}  Bi-gams : {}\" \\\n",
    "    .format(classtype, sorted(map2.items(), key = lambda x: x[1], reverse = True)[:15])\n",
    "    print\n",
    "    print \"Most popular {} Tri-grams : {}\" \\\n",
    "    .format(classtype, sorted(map3.items(), key=lambda x: x[1], reverse = True)[:15])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unigrams for Neutral Tweets : 5003\n",
      "\n",
      "Total Bi-grams for Neutral Tweets: 12261\n",
      "\n",
      "Total Tri-grams for Neutral Tweets: 11949\n",
      "\n",
      "Most popular Neutral Uni-grams : [((u'#gopdebate',), 940), ((u'?',), 348), ((u'#gopdebates',), 240), ((u'last',), 224), ((u'trump',), 219), ((u'!',), 184), ((u'debate',), 164), ((u'&',), 122), ((u'night',), 122), ((u'candidates',), 112), ((u'-',), 112), ((u'will',), 103), ((u'\\U0001f1fa\\U0001f1f8',), 103), ((u'@realdonaldtrump',), 102), ((u'one',), 99)]\n",
      "\n",
      "Most Popular Neutral  Bi-gams : [((u'last', u'night'), 105), ((u'last', u'nights'), 88), ((u'\\U0001f1fa\\U0001f1f8', u'#gopdebate'), 85), ((u'nights', u'#gopdebate'), 53), ((u'fox', u'news'), 50), ((u'#gopdebate', u'last'), 47), ((u'donald', u'trump'), 35), ((u'#gopdebate', u'?'), 32), ((u'gop', u'debate'), 32), ((u'?', u'#gopdebate'), 30), ((u'cruz', u'trump'), 30), ((u'rubio', u'\\U0001f1fa\\U0001f1f8'), 28), ((u'get', u'rid'), 28), ((u'together', u'expose'), 28), ((u'trump', u'need'), 28)]\n",
      "\n",
      "Most popular Neutral Tri-grams : [((u'last', u'nights', u'#gopdebate'), 53), ((u'#gopdebate', u'last', u'night'), 38), ((u'job', u'get', u'rid'), 28), ((u'rubio', u'\\U0001f1fa\\U0001f1f8', u'#gopdebate'), 28), ((u'expose', u'set', u'job'), 28), ((u'cruz', u'trump', u'need'), 28), ((u'think', u'cruz', u'trump'), 28), ((u'together', u'expose', u'set'), 28), ((u'rid', u'bush', u'rubio'), 28), ((u'need', u'band', u'together'), 28), ((u'\\U0001f1fa\\U0001f1f8', u'#gopdebate', u'#g'), 28), ((u'band', u'together', u'expose'), 28), ((u'bush', u'rubio', u'\\U0001f1fa\\U0001f1f8'), 28), ((u'set', u'job', u'get'), 28), ((u'trump', u'need', u'band'), 28)]\n"
     ]
    }
   ],
   "source": [
    "#initialize the neutral tweets, and their respective maps.  We use a pre-initialized map, because we want to use\n",
    "#apply on the dataframe, and this way nothing needs to be returned by the counting function.\n",
    "\n",
    "neutral_tweets = XyN_gram[XyN_gram.sentiment == 0]\n",
    "neutral_uni_grams = defaultdict(int)\n",
    "neutral_bi_grams = defaultdict(int)\n",
    "neutral_tri_grams = defaultdict(int)\n",
    "\n",
    "n_gram_counter(neutral_tweets, neutral_uni_grams, neutral_bi_grams, neutral_tri_grams, \"Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unigrams for Positive Tweets : 3459\n",
      "\n",
      "Total Bi-grams for Positive Tweets: 9017\n",
      "\n",
      "Total Tri-grams for Positive Tweets: 8893\n",
      "\n",
      "Most popular Positive Uni-grams : [((u'#gopdebate',), 764), ((u'trump',), 279), ((u'!',), 235), ((u'@realdonaldtrump',), 227), ((u'\\U0001f1fa\\U0001f1f8',), 220), ((u'#gopdebates',), 212), ((u'debate',), 144), ((u'last',), 132), ((u'&',), 115), ((u'fox',), 110), ((u'rubio',), 107), ((u'news',), 105), ((u'night',), 102), ((u'cruz',), 97), ((u'?',), 92)]\n",
      "\n",
      "Most Popular Positive  Bi-gams : [((u'\\U0001f1fa\\U0001f1f8', u'#gopdebate'), 197), ((u'fox', u'news'), 91), ((u'last', u'night'), 80), ((u'@realdonaldtrump', u's'), 60), ((u's', u'ratings'), 58), ((u'youre', u'raising'), 55), ((u'thanks', u'fox'), 55), ((u'raising', u'@realdonaldtrump'), 55), ((u'news', u'youre'), 55), ((u'ratings', u'\\U0001f1fa\\U0001f1f8'), 54), ((u'get', u'rid'), 51), ((u'rubio', u'\\U0001f1fa\\U0001f1f8'), 50), ((u'together', u'expose'), 50), ((u'trump', u'need'), 50), ((u'set', u'job'), 50)]\n",
      "\n",
      "Most popular Positive Tri-grams : [((u'@realdonaldtrump', u's', u'ratings'), 58), ((u'thanks', u'fox', u'news'), 55), ((u'fox', u'news', u'youre'), 55), ((u'youre', u'raising', u'@realdonaldtrump'), 55), ((u'news', u'youre', u'raising'), 55), ((u'raising', u'@realdonaldtrump', u's'), 55), ((u'ratings', u'\\U0001f1fa\\U0001f1f8', u'#gopdebate'), 54), ((u's', u'ratings', u'\\U0001f1fa\\U0001f1f8'), 54), ((u'set', u'job', u'get'), 50), ((u'job', u'get', u'rid'), 50), ((u'rubio', u'\\U0001f1fa\\U0001f1f8', u'#gopdebate'), 50), ((u'expose', u'set', u'job'), 50), ((u'cruz', u'trump', u'need'), 50), ((u'think', u'cruz', u'trump'), 50), ((u'together', u'expose', u'set'), 50)]\n"
     ]
    }
   ],
   "source": [
    "#Ok let's do that again, for positive tweets now\n",
    "\n",
    "positive_tweets = XyN_gram[XyN_gram.sentiment == 1]\n",
    "positive_uni_grams = defaultdict(int)\n",
    "positive_bi_grams = defaultdict(int)\n",
    "positive_tri_grams = defaultdict(int)\n",
    "\n",
    "n_gram_counter(positive_tweets, positive_uni_grams, positive_bi_grams, positive_tri_grams, \"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unigrams for Negative Tweets : 8948\n",
      "\n",
      "Total Bi-grams for Negative Tweets: 31465\n",
      "\n",
      "Total Tri-grams for Negative Tweets: 32424\n",
      "\n",
      "Most popular Negative Uni-grams : [((u'#gopdebate',), 2614), ((u'?',), 1162), ((u'\\U0001f1fa\\U0001f1f8',), 902), ((u'#gopdebates',), 862), ((u'fox',), 777), ((u'trump',), 663), ((u'&',), 520), ((u'!',), 504), ((u'news',), 490), ((u'just',), 452), ((u'debate',), 436), ((u'@realdonaldtrump',), 435), ((u'candidates',), 425), ((u'jeb',), 385), ((u'dont',), 382)]\n",
      "\n",
      "Most Popular Negative  Bi-gams : [((u'\\U0001f1fa\\U0001f1f8', u'#gopdebate'), 703), ((u'fox', u'news'), 464), ((u'?', u'#gopdebate'), 231), ((u'jeb', u'bush'), 211), ((u'chris', u'wallace'), 204), ((u'last', u'night'), 199), ((u'donald', u'trump'), 165), ((u'#gopdebate', u'#gopdebates'), 145), ((u'fair', u'&'), 139), ((u'&', u'balanced'), 137), ((u'\\U0001f1fa\\U0001f1f8', u'#gopdebates'), 132), ((u'bush', u'reminds'), 126), ((u'listen', u'\\U0001f1fa\\U0001f1f8'), 125), ((u'reminds', u'elevator'), 125), ((u'dont', u'listen'), 125)]\n",
      "\n",
      "Most popular Negative Tri-grams : [((u'fair', u'&', u'balanced'), 137), ((u'music', u'hear', u'dont'), 125), ((u'reminds', u'elevator', u'music'), 125), ((u'bush', u'reminds', u'elevator'), 125), ((u'listen', u'\\U0001f1fa\\U0001f1f8', u'#gopdebate'), 125), ((u'jeb', u'bush', u'reminds'), 125), ((u'dont', u'listen', u'\\U0001f1fa\\U0001f1f8'), 125), ((u'hear', u'dont', u'listen'), 125), ((u'elevator', u'music', u'hear'), 125), ((u'debate', u'\\U0001f1fa\\U0001f1f8', u'#gopdebate'), 113), ((u'?', u'#gopdebate', u'#gopdebates'), 110), ((u'obviously', u'trying', u'influence'), 107), ((u'makeup', u'republican', u'field'), 107), ((u'fox', u'news', u'obviously'), 107), ((u'news', u'obviously', u'trying'), 107)]\n"
     ]
    }
   ],
   "source": [
    "#Ok let's do that again, for the negative tweets now\n",
    "\n",
    "negative_tweets = XyN_gram[XyN_gram.sentiment == -1]\n",
    "negative_uni_grams = defaultdict(int)\n",
    "negative_bi_grams = defaultdict(int)\n",
    "negative_tri_grams = defaultdict(int)\n",
    "\n",
    "n_gram_counter(negative_tweets, negative_uni_grams, negative_bi_grams, negative_tri_grams, \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unigrams for ALL Tweets : 11982\n",
      "\n",
      "Total Bi-grams for ALL Tweets: 47531\n",
      "\n",
      "Total Tri-grams for ALL Tweets: 50506\n",
      "\n",
      "Most popular ALL Uni-grams : [((u'#gopdebate',), 4318), ((u'?',), 1602), ((u'#gopdebates',), 1314), ((u'\\U0001f1fa\\U0001f1f8',), 1225), ((u'trump',), 1161), ((u'fox',), 969), ((u'!',), 923), ((u'@realdonaldtrump',), 764), ((u'&',), 757), ((u'debate',), 744), ((u'last',), 706), ((u'news',), 663), ((u'candidates',), 618), ((u'just',), 551), ((u'like',), 514)]\n",
      "\n",
      "Most Popular ALL  Bi-gams : [((u'\\U0001f1fa\\U0001f1f8', u'#gopdebate'), 985), ((u'fox', u'news'), 605), ((u'last', u'night'), 384), ((u'?', u'#gopdebate'), 266), ((u'jeb', u'bush'), 228), ((u'donald', u'trump'), 221), ((u'chris', u'wallace'), 220), ((u'last', u'nights'), 213), ((u'#gopdebate', u'#gopdebates'), 173), ((u'debate', u'\\U0001f1fa\\U0001f1f8'), 153), ((u'fair', u'&'), 145), ((u'&', u'balanced'), 143), ((u'nights', u'#gopdebate'), 141), ((u'\\U0001f1fa\\U0001f1f8', u'#gopdebates'), 141), ((u'#gopdebate', u'last'), 129)]\n",
      "\n",
      "Most popular ALL Tri-grams : [((u'debate', u'\\U0001f1fa\\U0001f1f8', u'#gopdebate'), 153), ((u'fair', u'&', u'balanced'), 143), ((u'last', u'nights', u'#gopdebate'), 140), ((u'music', u'hear', u'dont'), 127), ((u'reminds', u'elevator', u'music'), 127), ((u'bush', u'reminds', u'elevator'), 127), ((u'listen', u'\\U0001f1fa\\U0001f1f8', u'#gopdebate'), 127), ((u'jeb', u'bush', u'reminds'), 127), ((u'dont', u'listen', u'\\U0001f1fa\\U0001f1f8'), 127), ((u'hear', u'dont', u'listen'), 127), ((u'elevator', u'music', u'hear'), 127), ((u'?', u'#gopdebate', u'#gopdebates'), 117), ((u'obviously', u'trying', u'influence'), 111), ((u'makeup', u'republican', u'field'), 111), ((u'fox', u'news', u'obviously'), 111)]\n"
     ]
    }
   ],
   "source": [
    "#Ok let's do that again, for the negative tweets now\n",
    "\n",
    "all_training_tweets = XyN_gram\n",
    "ALL_uni_grams = defaultdict(int)\n",
    "ALL_bi_grams = defaultdict(int)\n",
    "ALL_tri_grams = defaultdict(int)\n",
    "\n",
    "n_gram_counter(all_training_tweets, ALL_uni_grams, ALL_bi_grams, ALL_tri_grams, \"ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_pd_csv (how_much, filename, sentiment, *gramaps):\n",
    "    popular_grams = pd.DataFrame()\n",
    "    for gmap in gramaps:\n",
    "        sort = sorted(gmap.items(), key=lambda x:x[1], reverse = True)[:how_much]\n",
    "        df = pd.DataFrame([[row[0], row[1]] for row in sort], columns=[\"{}-grams\".format(len(row[0])), 'Count'])\n",
    "        popular_grams = pd.concat([popular_grams, df], axis=1)\n",
    "    popular_grams.to_csv('{}.csv'.format(filename), encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neutral_grams = [neutral_uni_grams, neutral_bi_grams, neutral_tri_grams]  \n",
    "pos_grams = [positive_uni_grams, positive_bi_grams ,positive_tri_grams]\n",
    "negative_grams = [negative_uni_grams, negative_bi_grams, negative_tri_grams]\n",
    "all_grams = [ALL_uni_grams, ALL_bi_grams, ALL_tri_grams]\n",
    "\n",
    "to_pd_csv(10, \"neutral_grams\",\"neutral\", *neutral_grams)\n",
    "to_pd_csv(10, \"positive_grams\", \"positive\", *pos_grams)\n",
    "to_pd_csv(10, \"negative_grams\", \"negative\", *negative_grams)\n",
    "to_pd_csv(10, \"all_grams\", \"ALL\", *all_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, lets use these gram maps to create some features finally.\n",
    "so what we want to do is :\n",
    "\n",
    "take each tweet.text and calculate the probability of that tweet existing as a neutral tweet.  \n",
    "we can use this feature to construct our first classifier, for neutral tweets.\n",
    "let's start by defining a function that calculates the probability of a tweet.  \n",
    "I will need to include smoothing, normalization and worry about over / underflow.\n",
    "actually the very first step is to transform our maps into maximum likliehood probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# maximum likliehood probabilities for neutral grams.\n",
    "# we will calculate maximum likliehood with smoothing, will use simple laplace k-smoothing, with k = 1\n",
    "\n",
    "\n",
    "def calculate_maximum_likliehood (gram_map, k_smoothing = 1, Prior_map = None):\n",
    "    MLE_estimates = {}\n",
    "    count_vocabulary = len(gram_map) # this is V, or the unique vocabulary\n",
    "    total_gram_count = sum(gram_map.values()) #the is the total count of all the grams\n",
    "    \n",
    "    if Prior_map != None:\n",
    "        prior_gram_vocabulary_count = len(Prior_map) # also V for smoothing on conditioned grams\n",
    "    \n",
    "    #figure out what kind of gram-map we have\n",
    "    keys = gram_map.keys()\n",
    "    if len(keys[0]) == 1: # we have unigrams\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key] + k_smoothing) / \\\n",
    "            float((count_vocabulary*k_smoothing) + total_gram_count)\n",
    "            # above will give MLE with smoothing = 1\n",
    "                \n",
    "    elif len(keys[0]) == 2: # We have bigrams, thus we sould condition on a previous unigram\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key] + k_smoothing) / \\\n",
    "            float((k_smoothing * prior_gram_vocabulary_count) + Prior_map[key[0],])\n",
    "    elif len(keys[0]) == 3: #should be 3 size, so condition on previous bi-gram\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key] + k_smoothing) / \\\n",
    "            float((k_smoothing * prior_gram_vocabulary_count) +Prior_map[key[:2]])\n",
    "    else: #should never get here\n",
    "        print \"whoa, what are you passing?\"\n",
    "        print key\n",
    "            \n",
    "\n",
    "    return MLE_estimates\n",
    "\n",
    "MLE_neutral_uni_gram = calculate_maximum_likliehood(neutral_uni_gram_map , 1)\n",
    "MLE_neutral_bi_gram = calculate_maximum_likliehood(neutral_bi_gram_map, 1, Prior_map=neutral_uni_gram_map)\n",
    "MLE_neutral_tri_gram = calculate_maximum_likliehood(neutral_tri_gram_map, 1, Prior_map=neutral_bi_gram_map)\n",
    "\n",
    "## sanity checks\n",
    "print len(MLE_neutral_uni_gram) == len(neutral_uni_gram_map)\n",
    "print len(MLE_neutral_bi_gram) == len(neutral_bi_gram_map)\n",
    "print len(MLE_neutral_tri_gram) == len(neutral_tri_gram_map)\n",
    "\n",
    "# should look reasonble?\n",
    "print MLE_neutral_bi_gram.values()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we have MLE for all the training data.  This is n-gram analysis on the positive data.\n",
    "NExt we need to reparse all the tweets, looking up their values in the MLE_pos gram maps.\n",
    "Take log probabilities of everything If a gram doesn't exist in the correct place, then we'll use smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def v_plus_n(grams):\n",
    "    total_unique_grams = len(grams) # this is V or the unique vocabulary for smoothing \n",
    "    total_gram_count = sum(grams.values()) #this is N, total number of counts of the vocabulary.\n",
    "    return float(total_unique_grams + total_gram_count)\n",
    "\n",
    "\n",
    "def probability_calculator (parsed_tweet, gram_size):\n",
    "    \n",
    "    if len(parsed_tweet) <1:  #this will catch any empty tweets I missed earlier.\n",
    "        print \"you have a NAN\"\n",
    "        print parsed_tweet\n",
    "        return \"NaN\"\n",
    "    \n",
    "    uni_VplusN = v_plus_n(neutral_uni_gram_map) # will use these values in smoothing\n",
    "    bi_VplusN = v_plus_n(neutral_bi_gram_map)\n",
    "    tri_VplusN = v_plus_n(neutral_tri_gram_map)\n",
    "        \n",
    "    # gram_map should correspond to gram_size i.E bi-grams, or tri-grams etc.\n",
    "    loop_range = range(len(parsed_tweet) - gram_size)\n",
    "    prob = 0\n",
    "    \n",
    "    if gram_size == 1: #unigrams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_neutral_uni_gram: #look up the probability value we've already calculated\n",
    "                prob += math.log(MLE_neutral_uni_gram[gram])\n",
    "            else:  #it's unseen so create a new probability with k-smoothing\n",
    "                #pass # penalize it with nothing\n",
    "                prob += math.log( 1.0 / uni_VplusN )  \n",
    "    \n",
    "    if gram_size == 2: #bi-grams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_neutral_bi_gram:\n",
    "                prob += math.log(MLE_neutral_bi_gram[gram])  #look up probability we've calculated\n",
    "            \n",
    "            else:  #condition the unseen bi-gram on the seen unigram.\n",
    "                #pass\n",
    "                if (gram[0],) in neutral_uni_gram_map:\n",
    "                    prob += math.log( 1.0 / (neutral_uni_gram_map[gram[0],] + len(neutral_uni_gram_map)))  \n",
    "                    \n",
    "                    # so if gram = ('this','cat'), and we have never seen that before.  we are\n",
    "                    # getting a probability that is: 1 / count('this') + count(unique_single grams)\n",
    "                    #obviously close to zero.  ....\n",
    "                else: #then if even the first part of this unseen bigram is not the unigram database, just do V+N\n",
    "                    prob += math.log(1.0 / bi_VplusN)\n",
    "    \n",
    "    if gram_size == 3: #tri-grams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_neutral_tri_gram:\n",
    "                prob += math.log(MLE_neutral_tri_gram[gram]) # look up prob we've already calculated\n",
    "            \n",
    "            else:\n",
    "                #pass\n",
    "                if gram[:2] in neutral_bi_gram_map:\n",
    "                    prob += math.log( 1.0 / (neutral_bi_gram_map[gram[:2]] + len(neutral_bi_gram_map)))\n",
    "                else:\n",
    "                    prob += math.log(1.0 / tri_VplusN)\n",
    "                             \n",
    "    probability = math.exp(prob) / len(parsed_tweet) # normalize by the number of grams in the tweet.\n",
    "    return probability\n",
    "   \n",
    "\n",
    "test_tweet = [u'ok', u',', u'can', u'cull', u'herd', u'two', u'#gopdebates', u'?', u'can', u'tell', u'pataki', u',', u'carson', u',', u'walker', u',', u'cruz', u'perry', u'go', u'home', u'?']\n",
    "print probability_calculator(test_tweet,1)\n",
    "print probability_calculator(test_tweet,2)\n",
    "print probability_calculator(test_tweet,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we want to make features using the probability calculator!! time to finally get positive probability features for all our tweets.  both training and testing need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_trainy = pd.DataFrame(X_train)  #have to convert the Series into a dataframe, in order to add columns\n",
    "X_trainy['neut-uni'] = X_trainy.text.apply(lambda x: probability_calculator(x, 1),1)\n",
    "X_trainy['neut-bi'] = X_trainy.text.apply(lambda x: probability_calculator(x,2),1)\n",
    "X_trainy['neut-tri'] = X_trainy.text.apply(lambda x: probability_calculator(x,3),1)\n",
    "print X_trainy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_trainy = pd.DataFrame(X_train)  #have to convert the Series into a dataframe, in order to add columns\n",
    "X_trainy['neut-uni'] = X_trainy.text.apply(lambda x: probability_calculator(x, 1),1)\n",
    "X_trainy['neut-bi'] = X_trainy.text.apply(lambda x: probability_calculator(x,2),1)\n",
    "X_trainy['neut-tri'] = X_trainy.text.apply(lambda x: probability_calculator(x,3),1)\n",
    "print X_trainy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#note that X_test is getting probability, but they are calculated based on X_train's probability model\n",
    "\n",
    "X_testy = pd.DataFrame(X_test) #have to convert the Series into a dataframe, in order to add columns\n",
    "X_testy['neut-uni'] = X_testy.text.apply(lambda x: probability_calculator(x, 1), 1)\n",
    "X_testy['neut-bi'] = X_testy.text.apply(lambda x: probability_calculator(x,2),1)\n",
    "X_testy['neut-tri'] = X_testy.text.apply(lambda x: probability_calculator(x,3),1)\n",
    "print X_testy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we can now make a classifier with these features.  This classifier will predict positive labels.  Let's try a few classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First let's drop the text tweets, they aren't helpful in actual classification\n",
    "X_train_go = X_trainy.drop(X_trainy.columns[0], axis =1)\n",
    "print X_train_go.head()\n",
    "X_test_go = X_testy.drop(X_testy.columns[0], axis =1)\n",
    "print X_test_go.head()\n",
    "\n",
    "#should be no reason to scale data, because we've normalized it all, it's all probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def f1_score_wrap (y_actual, y_predict):\n",
    "    return f1_score(y_actual, y_predict, average = \"weighted\")\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "def basic(clf, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    x_pred = clf.predict(X_train)\n",
    "    F1_train = f1_score_wrap(y_train, x_pred)\n",
    "    train_conf = confusion_matrix(y_train, x_pred)\n",
    "    \n",
    "    print \"training F1:\", F1_train\n",
    "    print\n",
    "    print \"training confusion:\\n\", train_conf\n",
    "    print\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    F1_score = f1_score_wrap(y_test, y_pred)\n",
    "    conf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print \"testing F1:\", F1_score\n",
    "    print\n",
    "    print \"confusion for testing\\n\", conf\n",
    "    \n",
    "basic(clf, X_train_go, X_test_go, y_train, y_test)\n",
    "print X_test_go.shape\n",
    "print X_train_go.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM = svm.SVC()\n",
    "basic(SVM, X_train_go, X_test_go, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "basic(lr, X_train_go, X_test_go, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "basic(gnb, X_train_go, X_test_go, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(n_estimators=100)\n",
    "basic(ada, X_train_go, X_test_go, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
